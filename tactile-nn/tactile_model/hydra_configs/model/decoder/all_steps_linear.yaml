name: 'MLP'
n_layer: 3
out_channels: [1024, 512, 117]  # the last one will be overridden by data.num_classes
init_dict: null  # {"method": "trunc_normal_"}
dropout: 0.0
use_bias: True
bias: null  # 0.1
batchnorm: null # null is for no batch norm
activation: "ReLU"  # last layer should have no activation
temporal_decoder: True
temporal_steps: 'all' # 'all' # or a list [t1, t2, ...]